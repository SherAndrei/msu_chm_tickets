\section{Метод Лебедева решения жестких систем.}

Напомним определение жесткой задачи
\begin{definition}
  Система $y'=Ay$, $y=(y_1,\ldots,y_n)^T$ называется жесткой
  если
  \[
    1)\ \text{Re}(\lambda_i(A))<0 \quad
    2)\ s = \frac{\max_i\abs{\text{Re}(\lambda_i(A))}}{\min_i\abs{\text{Re}(\lambda_i(A))}}\gg1
  \]
  Число $s$ называется \textit{числом жесткости}
\end{definition}

Было показано, что для таких систем не требуется выбор маленьких шагов $h$
для достижения требуемой точности локальной аппроксимации. При этом
взять слишком большие шаги мы не можем, так как должна соблюдаться устойчивость
задачи для сохранения сходимости. Хотим научиться подбирать большие шаги
так, чтобы сохранялась и устойчиовсть и желаемая аппроксимация.

Рассматривается задача
\[y'=Ay,\ y=(y_1,\ldots,y_N)^T,\ A\in\R^{N\times N}\]
Для нее выбирается явная схема с переменными шагами. Тогда за $N$ шагов
получим
\[y_N=y_{N-1}(1-h_NA)=y_0\underbrace{\prod_{k=1}^N(1-h_kA)}_{S}=y_0P_N(A)\]
\begin{lemma}
  Пусть собственные векторы матрицы $A$ образуют базис. Тогда
  \[\overline{\lim_{k\rightarrow\infty}}\norm{S^k}\leq\const\Leftrightarrow\abs{\lambda_i(S)}\leq1\]
\end{lemma}
\begin{proof}
  Найдем собственные числа матрицы $S$. так как собственные вектора матрицы $A$
  образуют базис, то вектор $y_0\in\R^N$ можно разложить по собственным
  векторам $y_0=\sum_{k=1}^Nc_iy^{(k)}$. Подействуем матрицей $S$ на такой вектор
  \[Sy_0=\prod_{k=1}^N(I-h_kA)y_0=\prod_{k=1}^N(I-h_kA)\sum_{i=1}^Nc_iy^{(i)}=\sum_{i=1}^N\prod_{k=1}^N(I-h_kA)c_iy^{(i)}\]
  Посмотрим на последнее выражение подробнее. В каждом из слагаемых
  участвует полином относительно матрицы $A$: $\prod_{k=1}^N(I-h_kA)y^{(i)}=(1+\ldots+h_kA^{N})y^{(i)}=(1+\ldots+h_k\lambda_i^{N})y^{(i)}$. Значит
  \[Sy_0=\sum_{i=1}^N\prod_{k=1}^N(1-h_k\lambda_i)c_iy^{(i)}\Rightarrow\lambda_i(S)=\prod_{k=1}^N(1-h_k\lambda_i(A))\]
  Тогда
  \[\overline{\lim_{k\rightarrow\infty}}\norm{S^k}\leq\const\Leftrightarrow\abs{\prod_{k=1}^N(1-h_k\lambda_i(A))}\leq1\]
\end{proof}

Из этой леммы следует, что для обеспечения сходимости мы можем брать $h$
больший, чем $h_{cou}$ до тех пор, пока $\abs{\prod_{k=1}^N(1-h_k\lambda_i(A))}\leq1$.

Мы хотим получить наибольший отрезок интегрирования и при этом сохранить сходимость.
Потребуем $\lambda_i\in[0,M]$, и тогда нам удастся переформулировать эти желания в
терминах полинома $P_N(x)$:

\begin{equation}\label{eq:lebedev_poly}
  \max_{x\in[0,M]}\abs{P_N(x)}\leq1,\ \sum_{i=1}^{N}h_i=-P_N'(0)\rightarrow\sup
\end{equation}

Мы хотим подобрать коэффициенты полинома - искомые $h_k$ - так, чтобы выполнялись
эти два условия. Нам известно, что на отрезке норма
многочлена Чебышева ограничена $1$, и по теореме Маркова
производная многочлена Чебышева вне соответствующего интервала максимальная
среди всех остальных полиномов. Многочлен Чебышева определен на $t\in[-1,1]$,
сделаем замену переменных на $x\in[0,M]$:
\[x=\frac{M+0}{2}+\frac{M-0}{2}t\Leftrightarrow t=\frac{2x-M}{M}\]
Таким образом, решением задачи \eqref{eq:lebedev_poly} является полином $T_N(\frac{2x-M}{M})$.
Так как нам известны корни многочлена Чебышева на $[0, M]$, то можем получить формулу для шагов
\[h_k=\left(\frac{M}{2}+\frac{M}{2}\cos\left(\frac{(2k-1)\pi}{2N}\right)\right)^{-1},\ k=1,\ldots,N\]
Вычислим соответствующую длину отрезка интегрирования, то есть значение $P_N'(0)$
\[T_N(x)=\cos{(N\arccos{x})}\Rightarrow T_N'(x)=\frac{N(\sin(N\arccos{x}))}{\sqrt{1-x^2}}=\frac{N\sin(N\arccos{x})}{\sin\arccos x}=NU_{N-1}(x)\]
Отсюда находим
\[\abs{\frac{d}{dx}T_N\left(\frac{2x-M}{M}\right)}_{x=0}=\abs{\frac{2N}{M}U_{N-1}(x)}_{x=-1}\]
Так как подставить $-1$ сразу не выходит, то посчитаем ассимптотику функции, записав в форме многочлена
\[U_{N-1}(x)=\frac{(x+\sqrt{x^2-1})^N-(x-\sqrt{x^2-1})^N}{2\sqrt{x^2-1}}=x^N\frac{(1+\frac{\sqrt{x^2-1}}{x})^N-(1-\frac{\sqrt{x^2-1}}{x})^N}{2\sqrt{x^2-1}}=\]
\[=x^N\frac{1+N\frac{\sqrt{x^2-1}}{x}+\bigO\left(\left(\frac{\sqrt{x^2-1}}{x}\right)^2\right)-1+N\frac{\sqrt{x^2-1}}{x}+\bigO\left(\left(\frac{\sqrt{x^2-1}}{x}\right)^2\right)}{2\sqrt{x^2-1}}=Nx^{N-1}+\bigO\left(x^{N-2}\sqrt{x^2-1}\right)\]
Таким образом, итоговый отрезок интегрирования
\[\abs{\frac{d}{dx}T_N\left(\frac{2x-M}{M}\right)}_{x=0}\approx\frac{2N^2}{M}\]
Для сравнения, если пользоваться постоянным шагом, который сохраняет сходимость $\left(h_{cou}=\frac{2}{M}\right)$,
то сможем проинтегрировать отрезок только длины $\frac{2N}{M}$, что в $N$ раз меньше, чем пользуясь Чебышевскими узлами.

Предложенная схема очень чувствительна к ошибкам округления. Почему?
Хоть и из требования $\norm{\prod_{k=1}^N(1-h_kA)}\leq1$,
для зафиксированного $k$ как величина $h_k$, так и $\norm{1-h_kA}$
может быть существенно больше единицы. Действительно, пусть
$e$ - собственный нормированный вектор матрицы $A$, соответствующий наибольшему $\lambda(A)=M$.
Найдем норму вектора $y=(I-h_NA)e$ для $i=N$:
\[h_N=\frac{1}{\frac{M}{2}+\frac{M}{2}\cos\left(\pi-\frac{\pi}{2N}\right)}=\frac{1}{\frac{M}{2}-\frac{M}{2}\cos\frac{\pi}{2N}}=\frac{2}{M\left(1-\left(1-\frac{1}{2}\frac{\pi^2}{4N^2}+\bigO(\frac{1}{N^4})\right)\right)}=\frac{2}{M\left(\frac{1}{2}\frac{\pi^2}{4N^2}+\bigO(\frac{1}{N^4})\right)}\approx\frac{N^2}{M}\]
Поэтому $\norm{y}=\norm{I-h_NA}e\approx\abs{1-\frac{N^2}{M}M}\approx N^2$.
Следовательно, для больших значений $N$ применение метода может
привести как к катастрафическому росту погрешности вычислений,
так и к переполнению разрядов. Эту проблему обходят перестановкой шагов
таким образом, чтобы на каждом шаге $\norm{\prod_{k}(1-h_kA)}\leq1$.

На данный момент математически обоснованы алгоритмы для $N=2^p3^q$.
Приведем без доказательства один из таких алгоритмов для $N=2^p$.
Нумеровку шагов проводят от большего к меньшему, то есть
\[h_{N+1-i}=\left(\frac{M}{2}+\frac{M}{2}\cos\frac{\pi(2i-1)}{2N}\right)^{-1},\ i=1,\ldots,N\]
Желаемая последовательность для $N=2$ имеет вид $(2, 1)$.
Пусть построена последовательность для $2^{p-1}$: $\{i_1,\ldots,i_{2^{p-1}}\}$.
Тогда последовательность для $2^{p}$ имеет вид
\[\{2^p+1-i_1,\ i_1,\ 2^p+1-i_2,\ i_2,\ldots,\ 2^p+1-i_{2^{p-1}},\ i_{2^{p-1}}\}\]
Например: $(3,2,4,1)$, $(6,3,7,2,5,4,8,1)$, $(11,6,14,3,10,7,15,2,12,5,13,4,9,8,16,1)$.
Таким образом, делается серия по правилу "малый - большой", самый малый шаг выполняется предпоследним,
а самый большой последним.
